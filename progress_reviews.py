from pathlib import Path
import xml.etree.ElementTree as ET
from datetime import datetime
import re

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import mpld3
from mpld3 import plugins

from astropy import table
from astropy import time
from astropy.io import fits
from astropy import constants as const
from astropy import units as u

import database_utilities as dbutils
from target_selection_tools import catalog_utilities as catutils
from target_selection_tools import apt
from lya_prediction_tools import lya, ism
import paths

#%% paths and preloads
data_folder = Path('/Users/parke/Google Drive/Research/STELa/data/uv_observations/hst-stis')
progress_folder = Path('progress_reviews')
status_folder = progress_folder / 'status_table_snapshots'

target_table = catutils.load_and_mask_ecsv(paths.selection_outputs / 'stage1_host_catalog.ecsv')
aptnames = apt.cat2apt_names(target_table['hostname'].tolist())
target_table['aptname'] = aptnames
target_table.add_index('aptname')

# load up the latest export of the obs progress table
path_main_table = dbutils.pathname_max(status_folder, 'Observation Progress*.xlsx')
main_table = pd.read_excel(path_main_table, keep_default_na=False)
main_table = table.Table.from_pandas(main_table)
main_table.add_index('Target')

#%% settings
saveplots = True

# targets = ['hd17156', 'k2-9', 'toi-1434', 'toi-1696', 'wolf503', 'hd207496']
# targets = ['toi-2015', 'toi-2079']
targets = 'any'

obs_filters = dict(targets=targets, after='2025-04-13', directory=data_folder)


#%% make a csv of the observation dates to drop into the main progress spreadsheet
pass

# most of this generated by ChatGPT

# make a new table based on the exported table for revised columns
column_map = (('Rank', 'rank'),
              ('Lya Visit\nin Phase II', 'lya planned'),
              ('Lya Visit\nLabels', 'lya visit'),
              ('Planned\nLya Obs', 'lya plandate'),
              ('Last Lya\nObs', 'lya obsdate'),
              ('FUV Visit\nin Phase II', 'fuv planned'),
              ('FUV Visit\nLabels', 'fuv visit'),
              ('Planned\nFUV Obs', 'fuv plandate'),
              ('Last FUV \nObs', 'fuv obsdate'))
oldcols, newcols = zip(*column_map)
main_table.rename_columns(oldcols, newcols)
new_table = main_table[newcols].copy()
new_table['lya planned'] = False
new_table['fuv planned'] = False
datecols = 'lya plandate', 'lya obsdate', 'fuv plandate', 'fuv obsdate'
for col in datecols:
        new_table[col] = ''
        new_table[col] = new_table[col].astype('object')

# parse the xml visit status export from STScI
latest_status_path = dbutils.pathname_max(status_folder, 'HST-17804-visit-status*.xml')
tree = ET.parse(latest_status_path)
root = tree.getroot()

# utility to extract earliest date from PlanWindow string
def parse_planwindow_date(text):
    """ Extracts the left-most date from a planWindow string. Expected input example: "Mar 31, 2025 - Apr 1, 2025 (2025.090 - 2025.091)" This function takes the first date (e.g. "Mar 31, 2025") and returns a datetime object. """
    try: # Split on the hyphen and take the first part
        date_part = text.split(" - ")[0].strip() # Parse the date; expected format e.g. "Mar 31, 2025"
        parsed_date = datetime.strptime(date_part, "%b %d, %Y")
        return parsed_date
    except Exception as e:
        return None

# Iterate over each visit element in the visit status, find the appropriate row, and update dates
records = []
for visit in root.findall('visit'):
    visit_label = visit.attrib.get('visit')
    lya_mask = np.char.count(main_table['lya visit'], visit_label)
    fuv_mask = np.char.count(main_table['fuv visit'], visit_label)
    if sum(lya_mask) > 0:
        stage = 'lya'
        i, = np.nonzero(lya_mask)
    elif sum(fuv_mask) > 0:
        stage = 'fuv'
        i, = np.nonzero(fuv_mask)
    else:
        raise ValueError('Visit label not found.')

    # mark observation as in the phase II
    new_table[f'{stage} planned'][i] = True

    plancol = f'{stage} plandate'
    obscol = f'{stage} obsdate'

    # Get the status text (if available)
    status_elem = visit.find('status')
    status = status_elem.text.strip() if status_elem is not None else ""

    # Get all planWindow elements (if any)
    plan_windows = visit.findall('planWindow')

    # Check if this visit is flagged as "not a candidate..."
    if status == "Executed":
        # For executed visits use the startTime as the actual observation date.
        start_time_elem = visit.find('startTime')
        if start_time_elem is not None and start_time_elem.text:
            actual_obs, = re.findall(r'\w{3} \d+, \d{4}', start_time_elem.text)
            new_table[obscol][i] = actual_obs
    else:
        # For visits that have not executed (Scheduled, Flight Ready, Implementation, etc.)
        # if planWindow elements exist, extract the earliest possible observation date.
        dates = []
        for pw in plan_windows:
            if pw.text:
                dt = parse_planwindow_date(pw.text)
                if dt:
                    dates.append(dt)
        if dates:
            earliest_date = min(dates)
            # Format the date as "Mon DD, YYYY" (e.g., "Mar 31, 2025")
            earliest_possible = earliest_date.strftime("%b %d, %Y")
            new_table[plancol][i] = earliest_possible

# mark updates
sorted_cols = list(newcols[:1])
for col in newcols[1:]:
    if 'date' in col:
        strcol = []
        for item in main_table[col]:
            stritem = '' if item == '' else item.strftime("%b %d, %Y")
            strcol.append(stritem)
        strcol = table.Column(strcol)
        new_table[col + ' old'] = strcol
    else:
        new_table[col + ' old'] = main_table[col]
    new_table[col + ' updated'] = new_table[col] != new_table[col + ' old']
    sorted_cols.extend((col, col + ' old', col + ' updated'))
new_table = new_table[sorted_cols]

today = datetime.today().strftime("%Y-%m-%d")
new_table.write(status_folder / f'visit_dates_for_copy-paste_{today}.csv', overwrite=True)


#%% make cumulative progress plots

for stage in ['lya', 'fuv']:
    mask = new_table[f'{stage} planned']
    n = np.sum(mask)
    ivec = np.arange(n) + 1
    datelists = []
    for datekey in ['obsdate', 'plandate']:
        date_strings = new_table[f'{stage} {datekey}'][mask]
        dates = []
        for datestr in date_strings:
            if datestr in ['', 'SNAP']:
                continue
            try:
                date = time.Time(datestr)
            except ValueError:
                _ = datetime.strptime(datestr, "%b %d, %Y")
                date = time.Time(_)
            dates.append(date)
        if dates:
            dates = time.Time(sorted(dates))
        datelists.append(dates)
    obsdates, plandates = datelists

    plt.figure()
    nobs = len(obsdates)
    nplan = len(plandates)
    if obsdates:
        plt.plot(obsdates.decimalyear, ivec[:nobs], 'k-', lw=2)
    plt.plot(plandates.decimalyear, ivec[nobs:nobs+nplan], '--', lw=2, color='0.5')
    plt.axhline(n, color='C2', lw=2)
    plt.xlim(2025.1, 2026.5)
    plt.ylim(-5, 135)
    plt.xlabel('Date')
    plt.ylabel(f'{stage.upper()} Observations Executed')
    plt.tight_layout()
    plt.savefig(progress_folder / f'{stage} progress chart.pdf')
    plt.savefig(progress_folder / f'{stage} progress chart.png', dpi=300)


#%% check acquisitions

rawfiles = dbutils.find_data_files('raw', instruments='hst-stis-mirvis', **obs_filters)

stages = ['coarse', 'fine', '0.2x0.2']
for file in rawfiles:
    fig, axs = plt.subplots(1, 3, figsize=[7,3])
    h = fits.open(file)
    for i, ax in enumerate(axs):
        data = h['sci', i+1].data
        ax.imshow(data)
        ax.set_title('')
    fig.suptitle(file.name)
    fig.supxlabel('dispersion')
    fig.supylabel('spatial')
    fig.tight_layout()


#%% plot extraction locations

fltfiles = dbutils.find_data_files('flt', instruments='hst-stis', **obs_filters)

for ff in fltfiles:
    img = fits.getdata(ff, 1)
    f1 = dbutils.modify_file_label(ff, 'x1d')
    td = fits.getdata(f1, 1)
    fig = plt.figure()
    plt.imshow(np.cbrt(img), aspect='auto')
    plt.title(ff.name)

    x = np.arange(img.shape[1]) + 0.5
    y = td['extrlocy']
    ysz = td['extrsize']
    plt.plot(x, y.T, color='w', lw=0.5, alpha=0.5)
    for _y, _ysz in zip(y, ysz):
        plt.fill_between(x, _y - _ysz/2, _y + _ysz/2, color='w', lw=0, alpha=0.5)
    for ibk in (1,2):
        off, sz = td[f'bk{ibk}offst'], td[f'bk{ibk}size']
        ym = y + off[:,None]
        y1, y2 = ym - sz[:,None]/2, ym + sz[:,None]/2
        for yy1, yy2 in zip(y1, y2):
            plt.fill_between(x, yy1, yy2, color='0.5', alpha=0.5, lw=0)

    if saveplots:
        dpi = fig.get_dpi()
        fig.set_dpi(150)
        plugins.connect(fig, plugins.MousePosition(fontsize=14))
        htmlfile = str(ff).replace('_flt.fits', '_plot-extraction.html')
        mpld3.save_html(fig, htmlfile)
        fig.set_dpi(dpi)


#%% plot spectra, piggyback Lya flux (O-C)/sigma

vstd = (lya.wgrid_std/1215.67/u.AA - 1)*const.c.to('km s-1')
x1dfiles = dbutils.find_data_files('x1d', instruments='hst-stis', **obs_filters)


for xf in x1dfiles:
    data = fits.getdata(xf, ext=1)
    order = 36 if 'e140m' in xf.name else 0
    spec = {}
    for name in data.names:
        spec[name.lower()] = data[name][order]

    # shift to velocity frame
    name = dbutils.parse_filename(xf)['target']
    rv = target_table.loc[name.upper()]['st_radv'] * u.km/u.s
    v = (spec['wavelength']/1215.67 - 1) * const.c - rv
    v = v.to_value('km s-1')

    fig = plt.figure()
    plt.title(xf.name)
    plt.step(v, spec['flux'], color='C0', where='mid')
    plt.xlim(-500, 500)

    # for plotting airglow
    size_scale = spec['extrsize'] / (spec['bk1size'] + spec['bk2size'])
    flux_factor = spec['flux'] / spec['net']
    z = spec['net'] == 0
    if np.any(z):
        raise NotImplementedError
    # i = np.arange(z.shape[1])
    # fi = np.interp(i[z], i[~z], ff[~z])
    # flux_factor[z] = fi
    bk_flux = spec['background'] * flux_factor * size_scale
    plt.step(v, bk_flux, color='C2', where='mid')

    plt.axhline(3e-14, color='0.5', lw=1, ls=':')

    # predicted lines
    ylim = plt.ylim()
    itarget = target_table.loc_indices[name.upper()]
    predicted_fluxes = []
    for pct in (-34, 0, +34):
        n_H = ism.ism_n_H_percentile(50 + pct)
        lya_factor = lya.lya_factor_percentile(50 - pct)
        profile, = lya.lya_at_earth_auto(target_table[[itarget]], n_H, lya_factor=lya_factor, default_rv='ism')
        plt.plot(vstd - rv, profile, color='0.5', lw=1)
        predicted_flux = np.trapz(profile, lya.wgrid_std)
        predicted_fluxes.append(predicted_flux)

    # line integral and (O-C)/sigma
    mask = bk_flux > 1e-14
    flux = spec['flux']
    flux[mask] = 0
    wmask = (v > -400) & (v < 400)
    O = np.trapz(flux[wmask], spec['wavelength'][wmask])
    C = predicted_fluxes[1].to_value('erg s-1 cm-2')
    sigma = max(predicted_fluxes) - predicted_fluxes[1]
    O_C_s = (O - C) / sigma.to_value('erg s-1 cm-2')
    print(f'{name}:')
    print(f'\tpredicted flux: {O:.2e}' )
    print(f'\t(O-C)/sigma: {O_C_s:.2f}')

    plt.ylim(ylim)
    plt.xlabel('Velocity in System Frame (km s-1)')
    plt.ylabel('Flux Density (cgs)')
    plt.legend(('signal', 'background'))

    if saveplots:
        pngfile = str(xf).replace('_x1d.fits', '_plot-spec.png')
        fig.savefig(pngfile, dpi=300)

        dpi = fig.get_dpi()
        fig.set_dpi(150)
        plugins.connect(fig, plugins.MousePosition(fontsize=14))
        htmlfile = str(xf).replace('_x1d.fits', '_plot-spec.html')
        mpld3.save_html(fig, htmlfile)
        fig.set_dpi(dpi)